{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Deep Learning in Biomedicine**\n",
    "#### **Final Project:** Classification of Chest X-ray Images and Patient Metadata Using Multi-Modal Model\n",
    "**Team 4**: Alexander Sternfeld, Silvia Romanato and Antoine Bonnet\n",
    "\n",
    "\n",
    "Here is a table with the models we will train:\n",
    "| Model | Vision | Tabular |\n",
    "| --- | --- | --- |\n",
    "| 1 | - | FCN | \n",
    "| 2 | ResNet50 (CNN) | FCN | \n",
    "| 3 | ResNet50 (CNN) | - |\n",
    "| 4 | DenseNet (CNN) | FCN | \n",
    "| 5 | DenseNet (CNN) | - | \n",
    "| 6 | Vision Transformer (ViT) | FCN | \n",
    "| 7 | Vision Transformer (ViT)| - | \n",
    "\n",
    "We use the same fully-connected network (FCN) for all models. \n",
    "\n",
    "### **Some ideas for the encoders**\n",
    "\n",
    "https://github.com/naity/image_tabular \n",
    "\n",
    "<img src=\"../figures/joint_encoders.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils import *\n",
    "from models import *\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torchvision.models import densenet121, DenseNet121_Weights, ResNet50_Weights, resnet50\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: **Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>PerformedProcedureStepDescription</th>\n",
       "      <th>ViewPosition</th>\n",
       "      <th>Rows</th>\n",
       "      <th>Columns</th>\n",
       "      <th>StudyDate</th>\n",
       "      <th>StudyTime</th>\n",
       "      <th>ProcedureCodeSequence_CodeMeaning</th>\n",
       "      <th>ViewCodeSequence_CodeMeaning</th>\n",
       "      <th>PatientOrientationCodeSequence_CodeMeaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n",
       "      <td>10000032</td>\n",
       "      <td>50414267</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>PA</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>21800506</td>\n",
       "      <td>213014.531</td>\n",
       "      <td>CHEST (PA AND LAT)</td>\n",
       "      <td>postero-anterior</td>\n",
       "      <td>Erect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       dicom_id  subject_id  study_id  \\\n",
       "0  02aa804e-bde0afdd-112c0b34-7bc16630-4e384014    10000032  50414267   \n",
       "\n",
       "  PerformedProcedureStepDescription ViewPosition  Rows  Columns  StudyDate  \\\n",
       "0                CHEST (PA AND LAT)           PA  3056     2544   21800506   \n",
       "\n",
       "    StudyTime ProcedureCodeSequence_CodeMeaning ViewCodeSequence_CodeMeaning  \\\n",
       "0  213014.531                CHEST (PA AND LAT)             postero-anterior   \n",
       "\n",
       "  PatientOrientationCodeSequence_CodeMeaning  \n",
       "0                                      Erect  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Info from the images\n",
    "info_jpg = pd.read_csv('../real_data/mimic-cxr-2.0.0-metadata.csv')\n",
    "info_jpg.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>50414267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  study_id  Atelectasis  Cardiomegaly  Consolidation  Edema  \\\n",
       "0    10000032  50414267          NaN           NaN            NaN    NaN   \n",
       "\n",
       "   Enlarged Cardiomediastinum  Fracture  Lung Lesion  Lung Opacity  \\\n",
       "0                         NaN       NaN          NaN           NaN   \n",
       "\n",
       "   No Finding  Pleural Effusion  Pleural Other  Pneumonia  Pneumothorax  \\\n",
       "0         1.0               NaN            NaN        NaN           NaN   \n",
       "\n",
       "   Support Devices  \n",
       "0              NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_data = pd.read_csv('../real_data/mimic-cxr-2.0.0-chexpert.csv')\n",
    "labels_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Visual Encoders**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data are unbalanced, therefore we need to implement a weighted sample or the weighted loss function:\n",
    "\n",
    "# from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# # Calculate weights for each sample\n",
    "# weights = 1. / torch.tensor([class_counts[label] for label in labels], dtype=torch.float)\n",
    "# sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "# # Use this sampler in the DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "\n",
    "\n",
    "######### We are trying to perform the weighted loss function firstÂ #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = list_images('../real_data/files')\n",
    "image_labels_mapping = create_image_labels_mapping(image_files, labels_data, info_jpg)\n",
    "df = pd.DataFrame.from_dict(image_labels_mapping, orient='index').reset_index()\n",
    "df['dicom_id'] = df['index'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "split = pd.read_csv('../real_data/mimic-cxr-2.0.0-split.csv')\n",
    "\n",
    "# merge the two dataframes on subject_id and study_id\n",
    "df = pd.merge(df, split, on=['subject_id', 'study_id', 'dicom_id'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train and test dataframes\n",
    "train_df = df[df['split'] == 'train']\n",
    "train_paths = train_df['index'].tolist()\n",
    "train_labels = train_df.iloc[:, 1:15].values.tolist()\n",
    "train_dict = create_image_labels_mapping(image_files, labels_data, info_jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df[df['split'] == 'test']\n",
    "test_paths = test_df['index'].tolist()\n",
    "test_labels = test_df.iloc[:, 1:15].values.tolist()\n",
    "test_dict = create_image_labels_mapping(image_files, labels_data, info_jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. **ResNet** (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior Anterior view:  torch.Size([3, 256, 256])\n",
      "Lateral view torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Define your transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),                                               # Resize images to the size expected by ResNet\n",
    "    transforms.ToTensor(),                                                       # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet mean and std\n",
    "])\n",
    "\n",
    "train_dataset = MedicalImagesDataset(train_dict, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)  \n",
    "test_dataset = MedicalImagesDataset(test_dict, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Iterate over batches of image,label pairs\n",
    "for i, (pa_images, lateral_images, labels) in enumerate(train_dataset):\n",
    "    print('Posterior Anterior view: ', pa_images.shape)\n",
    "    print('Lateral view', lateral_images.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# class_counts = Counter()\n",
    "# for _, _, labels in dataloader:\n",
    "#     labels = torch.argmax(labels, dim=1)\n",
    "#     class_counts.update(labels.tolist())\n",
    "# print(class_counts)\n",
    "\n",
    "class_counts = {8: 452, 0: 173, 1: 77, 7: 68, 9: 27, 6: 17, 3: 17, \n",
    "                5: 16, 11: 14, 2: 14, 4: 9, 12: 7, 13: 3, 10: 2}\n",
    "\n",
    "# Plot the class distribution\n",
    "plt.bar(class_counts.keys(), class_counts.values())\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of instances')\n",
    "plt.title('Class Distribution in Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this distrbution we can see that the data is imbalanced. We will use the `class_weight` on the cross-entropy loss to account for this. <br>\n",
    "Other possibilities are: \n",
    "1. **Resampling Your Dataset**:\n",
    "Oversampling the Minority Class: Increase the number of instances of underrepresented classes in your training set. This can be done by duplicating existing instances or generating synthetic instances (e.g., using SMOTE).\n",
    "Undersampling the Majority Class: Reduce the number of instances of overrepresented classes.\n",
    "2. **Data Augmentation**:\n",
    "Apply data augmentation techniques to the minority class to create synthetic data points. This is especially useful in image and audio processing tasks.\n",
    "3. **Change the Evaluation Metric**:\n",
    "Use evaluation metrics like F1-score, precision-recall curve, ROC AUC, etc., which give a better sense of model performance on imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class weights are:  tensor([0.0022, 0.0058, 0.0130, 0.0147, 0.0370, 0.0588, 0.0588, 0.0625, 0.0714,\n",
      "        0.0714, 0.1111, 0.1429, 0.3333, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "class_counts = {8: 452, 0: 173, 1: 77, 7: 68, 9: 27, 6: 17, 3: 17, \n",
    "                5: 16, 11: 14, 2: 14, 4: 9, 12: 7, 13: 3, 10: 2}\n",
    "class_weights = 1. / torch.tensor(list(class_counts.values()), dtype=torch.float)\n",
    "print('The class weights are: ', class_weights)\n",
    "\n",
    "model = DualInputModel(model='resnet50', num_classes=14)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "num_epochs = 2\n",
    "verbose = False\n",
    "\n",
    "def train_model(model, num_epochs, train_dataloader, criterion, optimizer, device, verbose = False):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  \n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        for i, (x_pa, x_lateral, labels) in enumerate(train_dataloader):\n",
    "            # Forward pass\n",
    "            inputs = x_lateral.to(device), x_pa.to(device)\n",
    "            outputs = model(inputs[0], inputs[1])\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            if verbose:\n",
    "                print('Predicted: ', predicted, 'Labels: ', labels, 'Loss: ', loss.item())\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        epoch_accuracy = 100 * correct_predictions / total_predictions\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (x_pa, x_lateral, labels) in enumerate(test_dataloader):\n",
    "            inputs = x_lateral.to(device), x_pa.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs[0], inputs[1])\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "        test_accuracy = 100 * correct_predictions / total_predictions\n",
    "        print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "    \n",
    "    return test_accuracy, model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = train_model(model, num_epochs, train_dataloader, criterion, optimizer, device, verbose = False)\n",
    "model = test_model(model, test_dataloader, device)\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. **DenseNet** (CNN)\n",
    "\n",
    "[GitHub link](https://github.com/liuzhuang13/DenseNet)\n",
    "\n",
    "[Huggingface link](https://huggingface.co/docs/timm/models/densenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),                                               # Resize images to the size expected by ResNet\n",
    "    transforms.ToTensor(),                                                       # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet mean and std\n",
    "])\n",
    "\n",
    "train_dataset = MedicalImagesDataset(train_dict, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)  \n",
    "test_dataset = MedicalImagesDataset(test_dict, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Iterate over batches of image,label pairs\n",
    "# for i, (pa_images, lateral_images, labels) in enumerate(train_dataset):\n",
    "#     print('Posterior Anterior view: ', pa_images.shape)\n",
    "#     print('Lateral view', lateral_images.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the images is 224x224x3 times 2 (frontal and lateral images). <br>\n",
    "The input of the labels is 1x14 features, with binary values (0 or 1). These are categorical features. <br>\n",
    "The output of the model is 1x14 features, with binary values (0 or 1). <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class weights are:  tensor([0.0022, 0.0058, 0.0130, 0.0147, 0.0370, 0.0588, 0.0588, 0.0625, 0.0714,\n",
      "        0.0714, 0.1111, 0.1429, 0.3333, 0.5000])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_model() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/silviaromanato/Desktop/EPFL/MA3/DeepLearning/biomed-team4/multimodal/biomed.ipynb Cella 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/silviaromanato/Desktop/EPFL/MA3/DeepLearning/biomed-team4/multimodal/biomed.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/silviaromanato/Desktop/EPFL/MA3/DeepLearning/biomed-team4/multimodal/biomed.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/silviaromanato/Desktop/EPFL/MA3/DeepLearning/biomed-team4/multimodal/biomed.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m train_model(model, num_epochs, train_dataloader, criterion, optimizer, device, verbose \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/silviaromanato/Desktop/EPFL/MA3/DeepLearning/biomed-team4/multimodal/biomed.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m test_model(model, test_dataloader, device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/silviaromanato/Desktop/EPFL/MA3/DeepLearning/biomed-team4/multimodal/biomed.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mmodel.pth\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: train_model() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "class_counts = {8: 452, 0: 173, 1: 77, 7: 68, 9: 27, 6: 17, 3: 17, \n",
    "                5: 16, 11: 14, 2: 14, 4: 9, 12: 7, 13: 3, 10: 2}\n",
    "class_weights = 1. / torch.tensor(list(class_counts.values()), dtype=torch.float)\n",
    "print('The class weights are: ', class_weights)\n",
    "\n",
    "model = DualInputModel(model='densenet121', num_classes=14)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_model(model, num_epochs, train_dataloader, criterion, optimizer, device, verbose = False)\n",
    "test_model(model, test_dataloader, device)\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. **Vision Transformer** (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
