{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MIMIC-III**: Pre-processing\n",
    "\n",
    "This notebook contains the pre-processing of the [MIMIC-III](https://physionet.org/content/mimiciii/1.4/) dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils import *\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "from data import *\n",
    "from preprocessing import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **Tabular data**\n",
    "\n",
    "[MIMIC-IV Documentation](https://mimic.mit.edu/docs/iv/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tables that we use**\n",
    "\n",
    "* `Admissions`\n",
    "* `Patients`\n",
    "* `Services`\n",
    "\n",
    "\n",
    "**Tables that could be used:**\n",
    "* `emar`\n",
    "* `d_hcpcs.csv.gz`\n",
    "* `hpcsevents`\n",
    "* `labevents`\n",
    "* `microbiologyevents`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess admissions and patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Images data**\n",
    "\n",
    "\n",
    "The mimic-cxr-2.0.0-metadata.csv.gz file contains useful meta-data derived from the original DICOM files in MIMIC-CXR. The columns are:\n",
    "\n",
    "* **dicom_id** - An identifier for the DICOM file. The stem of each JPG image filename is equal to the dicom_id.\n",
    "* **PerformedProcedureStepDescription** - The type of study performed (\"CHEST (PA AND LAT)\", \"CHEST (PORTABLE AP)\", etc).\n",
    "* **ViewPosition** - The orientation in which the chest radiograph was taken (\"AP\", \"PA\", \"LATERAL\", etc).\n",
    "* **Rows** - The height of the image in pixels.\n",
    "* **Columns** - The width of the image in pixels.\n",
    "* **StudyDate** - An anonymized date for the radiographic study. All images from the same study will have the same date and time. Dates are anonymized, but chronologically consistent for each patient. Intervals between two scans have not been modified during de-identification.\n",
    "* **StudyTime** - The time of the study in hours, minutes, seconds, and fractional seconds. The time of the study was not modified during de-identification.\n",
    "* **ProcedureCodeSequence_CodeMeaning** - The human readable description of the coded procedure (e.g. \"CHEST (PA AND LAT)\". Descriptions follow Simon-Leeming codes [11].\n",
    "* **ViewCodeSequence_CodeMeaning** - The human readable description of the coded view orientation for the image (e.g. \"postero-anterior\", \"antero-posterior\", \"lateral\").\n",
    "* **PatientOrientationCodeSequence_CodeMeaning** - The human readable description of the patient orientation during the image acquisition. Three values are possible: \"Erect\", \"Recumbent\", or a null value (missing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info from the images\n",
    "info_jpg = pd.read_csv('../fake_data/mimic-cxr-2.0.0-metadata.csv')\n",
    "info_jpg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \"No Finding\" is the absence of any of the 13 descriptive labels and a check that the text does not mention a specified set of other common findings beyond those covered by the descriptive labels. Each label column contains one of four values: 1.0, -1.0, 0.0, or missing. These labels have the following interpretation:\n",
    "\n",
    "* **1.0** - The label was positively mentioned in the associated study, and is present in one or more of the corresponding images e.g. \"A large pleural effusion\"\n",
    "* **0.0** - The label was negatively mentioned in the associated study, and therefore should not be present in any of the corresponding images e.g. \"No pneumothorax.\"\n",
    "* **-1.0** - The label was either: (1) Explicit uncertainty or (2) Ambiguous language\n",
    "* **Missing** (empty element) - No mention of the label was made in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_data = pd.read_csv('../fake_data/mimic-cxr-2.0.0-chexpert.csv')\n",
    "labels_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_images(base_path):\n",
    "    \"\"\"\n",
    "    Recursively lists all image files starting from the base path.\n",
    "    Assumes that images have extensions typical for image files (e.g., .jpg, .jpeg, .png).\n",
    "    \"\"\"\n",
    "    image_files = []\n",
    "    for subdir, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_files.append(os.path.join(subdir, file))\n",
    "    return image_files\n",
    "\n",
    "image_files = list_images('../fake_data/files')\n",
    "image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_labels_mapping = create_image_labels_mapping(image_files, labels_data, info_jpg)\n",
    "        \n",
    "image_labels_mapping[image_files[0]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "class MedicalImagesDataset(Dataset):\n",
    "    def __init__(self, data_dict, size=224, transform=None):\n",
    "        self.data_dict = data_dict\n",
    "        self.transform = transform\n",
    "        self.size = size\n",
    "        self.classes = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', \n",
    "                        'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion', \n",
    "                        'Lung Opacity', 'No Finding', 'Pleural Effusion', \n",
    "                        'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices']\n",
    "        # Organize paths by subject_id and study_id\n",
    "        self.organized_paths = self._organize_paths()\n",
    "        # Filter out pairs where both images are None\n",
    "        self.organized_paths = {k: v for k, v in self.organized_paths.items() if v['PA'] is not None or v['Lateral'] is not None}\n",
    "\n",
    "    def _organize_paths(self):\n",
    "        organized = {}\n",
    "        for path in self.data_dict.keys():\n",
    "            parts = path.split(os.sep)\n",
    "            subject_id = parts[-3][1:]\n",
    "            study_id = parts[-2][1:]\n",
    "            key = (subject_id, study_id)\n",
    "\n",
    "            if key not in organized:\n",
    "                organized[key] = {'PA': None, 'Lateral': None}\n",
    "\n",
    "            view_position = self.data_dict[path]['ViewPosition']\n",
    "            if view_position in ['PA', 'Lateral']:\n",
    "                organized[key][view_position] = path\n",
    "\n",
    "        return organized\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.organized_paths)\n",
    "\n",
    "    def _load_and_process_image(self, path):\n",
    "        if path:\n",
    "            image = Image.open(path).convert('RGB')\n",
    "        else:\n",
    "            # Create a blank (black) image if path is None\n",
    "            image = Image.new('RGB', (self.size, self.size))\n",
    "\n",
    "        image = image.resize((self.size, self.size))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_study_pair = list(self.organized_paths.keys())[idx]\n",
    "        pa_path = self.organized_paths[subject_study_pair]['PA']\n",
    "        lateral_path = self.organized_paths[subject_study_pair]['Lateral']\n",
    "\n",
    "        # Load and process PA and Lateral images\n",
    "        pa_image = self._load_and_process_image(pa_path)\n",
    "        lateral_image = self._load_and_process_image(lateral_path)\n",
    "\n",
    "        # Use one of the available paths to get labels (assuming they are the same for both views)\n",
    "        labels_path = pa_path if pa_path else lateral_path\n",
    "\n",
    "        if not labels_path:\n",
    "            # Skip this patient if both PA and Lateral images are missing\n",
    "            return None\n",
    "\n",
    "        labels = self.data_dict[labels_path]\n",
    "        label_values = [labels[class_name] if not np.isnan(labels[class_name]) else 0 for class_name in self.classes]\n",
    "        label_tensor = torch.tensor(label_values, dtype=torch.float32)\n",
    "\n",
    "        return pa_image, lateral_image, label_tensor\n",
    "\n",
    "# Transform definition (if needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = MedicalImagesDataset(image_labels_mapping, transform=transform)\n",
    "dataloaders = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "for pa_images, lateral_images, labels in dataloaders:\n",
    "    print(pa_images.shape)\n",
    "    print(lateral_images.shape)\n",
    "    print(labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over batches of image,label pairs\n",
    "for i, (images, labels, view) in enumerate(dataloader):\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    print(view)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "silviasenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
